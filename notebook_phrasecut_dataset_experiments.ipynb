{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.endovis import Endovis2017\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "from general_utils import TrainingLogger, get_attribute, filter_args, log, training_config_from_cli_args\n",
    "from general_utils import AttributeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"endovis2017.yaml\"\n",
    "experiment_id = 0\n",
    "\n",
    "yaml_config = yaml.load(open(f'experiments/{experiment_name}'), Loader=yaml.SafeLoader) \n",
    "\n",
    "config = yaml_config['configuration']\n",
    "\n",
    "config = {**config, **yaml_config['individual_configurations'][experiment_id]}\n",
    "\n",
    "config = AttributeDict(config) #a new type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cls = get_attribute(config.dataset)  #😉 dataset args from config\n",
    "_, dataset_args, _ = filter_args(config, inspect.signature(dataset_cls).parameters) #😉 dataset args filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': 'train',\n",
       " 'mask': 'text_and_crop_blur_highlight352',\n",
       " 'image_size': 352,\n",
       " 'aug': '1new',\n",
       " 'with_visual': True,\n",
       " 'negative_prob': 0.2}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = Endovis2017(**dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12281"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1280, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropped_train\\instrument_dataset_1\\clipseg_masks\\frame000_bipolar_forceps.png\n",
      "(1024, 1280, 3)\n",
      "(1024, 1280)\n",
      "torch.Size([3, 352, 352])\n",
      "torch.Size([352, 352])\n",
      "./datasets/Endovis2017/visual_prompt\\no_combination\\instrument_segmentation\\img\\bipolar_forceps.jpg\n",
      "./datasets/Endovis2017/visual_prompt\\no_combination\\instrument_segmentation\\mask\\bipolar_forceps.jpg\n",
      "(339, 512, 3)\n",
      "(339, 512, 3)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_data[\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[1;32md:\\PhD\\repositories\\clipseg\\datasets\\endovis.py:220\u001b[0m, in \u001b[0;36mEndovis2017.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    217\u001b[0m             mask_mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask\n\u001b[0;32m    218\u001b[0m             label_add \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 220\u001b[0m         masked_img_s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(blend_image_segmentation(img_s, seg_s, mode\u001b[39m=\u001b[39;49mmask_mode, image_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_size)[\u001b[39m0\u001b[39m])\n\u001b[0;32m    221\u001b[0m         vis_s \u001b[39m=\u001b[39m label_add \u001b[39m+\u001b[39m [masked_img_s, \u001b[39mTrue\u001b[39;00m] \u001b[39m#😉 if with_visual, phrase in samples_by_phrase and mask is not  separate or text-and_separate, vis_s is [phrase, masked_img_s, True], masked_img_s = a blend of the image and segmentation, 🙋‍♂️True not sure\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m#😉 No visual, just normal ref expression segmentation training. and not training on phrase cut+. We are interested in this.\u001b[39;00m\n",
      "File \u001b[1;32md:\\PhD\\repositories\\clipseg\\datasets\\utils.py:46\u001b[0m, in \u001b[0;36mblend_image_segmentation\u001b[1;34m(img, seg, mode, image_size)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcrop_blur_highlight352\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     45\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mevaluation_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m img_preprocess\n\u001b[1;32m---> 46\u001b[0m     out  \u001b[39m=\u001b[39m [img_preprocess((\u001b[39mNone\u001b[39;49;00m, [img], [seg]), blur\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, center_context\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, bg_fac\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, image_size\u001b[39m=\u001b[39;49m\u001b[39m352\u001b[39;49m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()]          \n\u001b[0;32m     47\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     48\u001b[0m     out \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mstack([seg[:, :]]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)]\n",
      "File \u001b[1;32md:\\PhD\\repositories\\clipseg\\evaluation_utils.py:114\u001b[0m, in \u001b[0;36mimg_preprocess\u001b[1;34m(batch, blur, grayscale, center_context, rect, rect_color, rect_width, brightness, bg_fac, colorize, outline, image_size)\u001b[0m\n\u001b[0;32m    111\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mcpu() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(img)\n\u001b[0;32m    112\u001b[0m mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mcpu() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(mask)\n\u001b[1;32m--> 114\u001b[0m img \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m brightness\n\u001b[0;32m    115\u001b[0m img_bl \u001b[39m=\u001b[39m img\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m blur \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# best 5\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Byte"
     ]
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_data \u001b[39m=\u001b[39m Endovis2017(train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m figure \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[0;32m      4\u001b[0m num_of_samples \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'train'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "num_of_samples = 5\n",
    "num_img_per_sample = 5\n",
    "rows, cols = num_of_samples, num_img_per_sample #😉 4 because I want to display img1, img2, semantic, opticalflow\n",
    "for i in range(0, num_of_samples):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label_map, flow_map, _ = training_data[sample_idx]\n",
    "\n",
    "    # print(img.shape)\n",
    "    # print(label_map.shape)\n",
    "    # print(flow_map.shape)\n",
    "\n",
    "    img = np.moveaxis(img.numpy(), 0, -1)\n",
    "    img = img.astype(np.uint8)\n",
    "    img1 = img[...,0:3]\n",
    "    img2 = img[...,3:6]\n",
    "\n",
    "\n",
    "    label_map = label_map.numpy()\n",
    "    sem1_color_img = transform_label_map_to_colour_segmentation(label_map[0,...])\n",
    "    sem2_color_img = transform_label_map_to_colour_segmentation(label_map[1,...])\n",
    "\n",
    "    \n",
    "    figure.add_subplot(rows, cols, (i*num_img_per_sample) + 1 )\n",
    "    plt.title(f\"{sample_idx}_img1\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img1)\n",
    "\n",
    "    figure.add_subplot(rows, cols, (i*num_img_per_sample) + 2)\n",
    "    plt.title(f\"{sample_idx}_img2\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img2)\n",
    "\n",
    "    figure.add_subplot(rows, cols, (i*num_img_per_sample) + 3)\n",
    "    plt.title(f\"{sample_idx}_sem1\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(sem1_color_img)\n",
    "\n",
    "    figure.add_subplot(rows, cols, (i*num_img_per_sample) + 4)\n",
    "    plt.title(f\"{sample_idx}_sem2\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(sem2_color_img)\n",
    "\n",
    "    figure.add_subplot(rows, cols, (i*num_img_per_sample) + 5)\n",
    "    plt.title(f\"{sample_idx}_flow\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(flow_map_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".CLIPSEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "830aef9cd6859694268c088411cb058faec24a5a65da1f26d3ab795eccfbf0f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
