{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PhD\\repositories\\clipseg\\.CLIPSEG\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "from general_utils import *\n",
    "from datasets.phrasecut import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"phrasecut.yaml\"\n",
    "experiment_id = 0\n",
    "\n",
    "yaml_config = yaml.load(open(f'experiments/{experiment_name}'), Loader=yaml.SafeLoader) \n",
    "\n",
    "config = yaml_config['configuration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (940108100.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    yaml_config['individual_configurations'][]\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "yaml_config['individual_configurations'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1,\n",
       " 'optimizer': 'torch.optim.AdamW',\n",
       " 'lr': 0.001,\n",
       " 'trainer': 'experiment_setup.train_loop',\n",
       " 'scorer': 'experiment_setup.score',\n",
       " 'model': 'models.clipseg.CLIPDensePredT',\n",
       " 'lr_scheduler': 'cosine',\n",
       " 'T_max': 20000,\n",
       " 'eta_min': 0.0001,\n",
       " 'max_iterations': 2,\n",
       " 'val_interval': None,\n",
       " 'dataset': 'datasets.phrasecut.PhraseCut',\n",
       " 'split_mode': 'pascal_test',\n",
       " 'split': 'train',\n",
       " 'mask': 'text_and_crop_blur_highlight352',\n",
       " 'image_size': 352,\n",
       " 'normalize': True,\n",
       " 'pre_crop_image_size': ['sample', 1, 1.5],\n",
       " 'aug': '1new',\n",
       " 'mix': True,\n",
       " 'prompt': 'shuffle+',\n",
       " 'norm_cond': True,\n",
       " 'mix_text_min': 0.0,\n",
       " 'out': 1,\n",
       " 'extract_layers': [3, 7, 9],\n",
       " 'reduce_dim': 64,\n",
       " 'depth': 3,\n",
       " 'fix_shift': False,\n",
       " 'loss': 'torch.nn.functional.binary_cross_entropy_with_logits',\n",
       " 'amp': True,\n",
       " 'name': 'rd64-uni',\n",
       " 'version': 'ViT-B/16',\n",
       " 'with_visual': True,\n",
       " 'negative_prob': 0.2,\n",
       " 'mix_text_max': 0.5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {**config, **yaml_config['individual_configurations'][experiment_id]}\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import AttributeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1,\n",
       " 'optimizer': 'torch.optim.AdamW',\n",
       " 'lr': 0.001,\n",
       " 'trainer': 'experiment_setup.train_loop',\n",
       " 'scorer': 'experiment_setup.score',\n",
       " 'model': 'models.clipseg.CLIPDensePredT',\n",
       " 'lr_scheduler': 'cosine',\n",
       " 'T_max': 20000,\n",
       " 'eta_min': 0.0001,\n",
       " 'max_iterations': 2,\n",
       " 'val_interval': None,\n",
       " 'dataset': 'datasets.phrasecut.PhraseCut',\n",
       " 'split_mode': 'pascal_test',\n",
       " 'split': 'train',\n",
       " 'mask': 'text_and_crop_blur_highlight352',\n",
       " 'image_size': 352,\n",
       " 'normalize': True,\n",
       " 'pre_crop_image_size': ['sample', 1, 1.5],\n",
       " 'aug': '1new',\n",
       " 'mix': True,\n",
       " 'prompt': 'shuffle+',\n",
       " 'norm_cond': True,\n",
       " 'mix_text_min': 0.0,\n",
       " 'out': 1,\n",
       " 'extract_layers': [3, 7, 9],\n",
       " 'reduce_dim': 64,\n",
       " 'depth': 3,\n",
       " 'fix_shift': False,\n",
       " 'loss': 'torch.nn.functional.binary_cross_entropy_with_logits',\n",
       " 'amp': True,\n",
       " 'name': 'rd64-uni',\n",
       " 'version': 'ViT-B/16',\n",
       " 'with_visual': True,\n",
       " 'negative_prob': 0.2,\n",
       " 'mix_text_max': 0.5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AttributeDict(config) #a new type\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import get_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = get_attribute(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'models.clipseg',\n",
       "              '__init__': <function models.clipseg.CLIPDensePredT.__init__(self, version='ViT-B/32', extract_layers=(3, 6, 9), cond_layer=0, reduce_dim=128, n_heads=4, prompt='fixed', extra_blocks=0, reduce_cond=None, fix_shift=False, learn_trans_conv_only=False, limit_to_clip_only=False, upsample=False, add_calibration=False, rev_activations=False, trans_conv=None, n_tokens=None, complex_trans_conv=False)>,\n",
       "              'forward': <function models.clipseg.CLIPDensePredT.forward(self, inp_image, conditional=None, return_features=False, mask=None)>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(model_cls)\n",
    "model_cls.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1,\n",
       " 'optimizer': 'torch.optim.AdamW',\n",
       " 'lr': 0.001,\n",
       " 'trainer': 'experiment_setup.train_loop',\n",
       " 'scorer': 'experiment_setup.score',\n",
       " 'model': 'models.clipseg.CLIPDensePredT',\n",
       " 'lr_scheduler': 'cosine',\n",
       " 'T_max': 20000,\n",
       " 'eta_min': 0.0001,\n",
       " 'max_iterations': 2,\n",
       " 'val_interval': None,\n",
       " 'dataset': 'datasets.phrasecut.PhraseCut',\n",
       " 'split_mode': 'pascal_test',\n",
       " 'split': 'train',\n",
       " 'mask': 'text_and_crop_blur_highlight352',\n",
       " 'image_size': 352,\n",
       " 'normalize': True,\n",
       " 'pre_crop_image_size': ['sample', 1, 1.5],\n",
       " 'aug': '1new',\n",
       " 'mix': True,\n",
       " 'prompt': 'shuffle+',\n",
       " 'norm_cond': True,\n",
       " 'mix_text_min': 0.0,\n",
       " 'out': 1,\n",
       " 'extract_layers': [3, 7, 9],\n",
       " 'reduce_dim': 64,\n",
       " 'depth': 3,\n",
       " 'fix_shift': False,\n",
       " 'loss': 'torch.nn.functional.binary_cross_entropy_with_logits',\n",
       " 'amp': True,\n",
       " 'name': 'rd64-uni',\n",
       " 'version': 'ViT-B/16',\n",
       " 'with_visual': True,\n",
       " 'negative_prob': 0.2,\n",
       " 'mix_text_max': 0.5}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_args() compares config with the keys of the __init__ varible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'version': <Parameter \"version='ViT-B/32'\">,\n",
       "              'extract_layers': <Parameter \"extract_layers=(3, 6, 9)\">,\n",
       "              'cond_layer': <Parameter \"cond_layer=0\">,\n",
       "              'reduce_dim': <Parameter \"reduce_dim=128\">,\n",
       "              'n_heads': <Parameter \"n_heads=4\">,\n",
       "              'prompt': <Parameter \"prompt='fixed'\">,\n",
       "              'extra_blocks': <Parameter \"extra_blocks=0\">,\n",
       "              'reduce_cond': <Parameter \"reduce_cond=None\">,\n",
       "              'fix_shift': <Parameter \"fix_shift=False\">,\n",
       "              'learn_trans_conv_only': <Parameter \"learn_trans_conv_only=False\">,\n",
       "              'limit_to_clip_only': <Parameter \"limit_to_clip_only=False\">,\n",
       "              'upsample': <Parameter \"upsample=False\">,\n",
       "              'add_calibration': <Parameter \"add_calibration=False\">,\n",
       "              'rev_activations': <Parameter \"rev_activations=False\">,\n",
       "              'trans_conv': <Parameter \"trans_conv=None\">,\n",
       "              'n_tokens': <Parameter \"n_tokens=None\">,\n",
       "              'complex_trans_conv': <Parameter \"complex_trans_conv=False\">})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(model_cls).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import filter_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'models.clipseg.CLIPDensePredT'>\n",
      "{'prompt': 'shuffle+', 'extract_layers': [3, 7, 9], 'reduce_dim': 64, 'fix_shift': False, 'version': 'ViT-B/16'}\n"
     ]
    }
   ],
   "source": [
    "model_cls = get_attribute(config.model) #ðŸ˜‰ model class from attribute\n",
    "print(model_cls)\n",
    "_, model_args, _ = filter_args(config, inspect.signature(model_cls).parameters)#ðŸ˜‰model args are filtered.\n",
    "print(model_args)\n",
    "model = model_cls(**model_args).cuda() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.phrasecut.PhraseCut'>\n",
      "{'split': 'train', 'mask': 'text_and_crop_blur_highlight352', 'image_size': 352, 'aug': '1new', 'with_visual': True, 'negative_prob': 0.2}\n"
     ]
    }
   ],
   "source": [
    "dataset_cls = get_attribute(config.dataset)  #ðŸ˜‰ dataset args from config\n",
    "print(dataset_cls)\n",
    "_, dataset_args, _ = filter_args(config, inspect.signature(dataset_cls).parameters) #ðŸ˜‰ dataset args filtered\n",
    "print(dataset_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", split, image_size=400, negative_prob=0, aug=None, aug_color=False, aug_crop=True,\n",
    "                 min_size=0, remove_classes=None, with_visual=False, only_visual=False, mask=None\n",
    "\n",
    " All possible arguents for phrasecut                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhraseHandler loading nar_count: D:\\PhD\\repositories\\clipseg\\third_party\\PhraseCutDataset\\data\\VGPhraseCut_v0\\name_att_rel_count.json\n",
      "Number of categories: 1272 / 3103, frequency thresh: 21 (excluding [INV] [UNK])\n",
      "Number of attributes: 593 / 12143, frequency thresh: 21 (excluding [INV] [UNK])\n",
      "Number of relationships: 126 / 3110, frequency thresh: 21 (excluding [INV] [UNK])\n",
      "RefVGLoader loading img_info: D:\\PhD\\repositories\\clipseg\\third_party\\PhraseCutDataset\\data\\VGPhraseCut_v0\\image_data_split.json\n",
      "RefVGLoader loading refer data\n",
      "RefVGLoader loading D:\\PhD\\repositories\\clipseg\\third_party\\PhraseCutDataset\\data\\VGPhraseCut_v0\\refer_train.json\n",
      "RefVGLoader preparing data\n",
      "split train: 71746 imgs, 310816 tasks\n",
      "RefVGLoader ready.\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_cls(**dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset PhraseCut (length: 310742)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train dataset {dataset.__class__.__name__} (length: {len(dataset)})') #ðŸ˜‰logs train dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\usuario/datasets/PhraseCut/VGPhraseCut_v0/images/2384227.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m dataset[\u001b[39m1\u001b[39;49m]\n",
      "File \u001b[1;32md:\\PhD\\repositories\\clipseg\\datasets\\phrasecut.py:322\u001b[0m, in \u001b[0;36mPhraseCut.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[0;32m    320\u001b[0m     sample_i, j \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_ids[i]\n\u001b[1;32m--> 322\u001b[0m     img, seg, phrase \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_sample(sample_i, j) \u001b[39m#ðŸ˜‰ Image, segmentation and phrase. ðŸ™‹â€â™‚ï¸ I am not sure what sample_i and j stand for. \u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnegative_prob \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m#ðŸ˜‰ Negative samples.  #ðŸ˜‰ sampling for negative probability\u001b[39;00m\n\u001b[0;32m    325\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mrand((\u001b[39m1\u001b[39m,))\u001b[39m.\u001b[39mitem() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnegative_prob: \u001b[39m#ðŸ˜‰ probability for negative probability.\u001b[39;00m\n",
      "File \u001b[1;32md:\\PhD\\repositories\\clipseg\\datasets\\phrasecut.py:285\u001b[0m, in \u001b[0;36mPhraseCut.load_sample\u001b[1;34m(self, sample_i, j)\u001b[0m\n\u001b[0;32m    282\u001b[0m         masks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [polygon2mask((img_ref_data[\u001b[39m'\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m'\u001b[39m], img_ref_data[\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m]), poly)] \u001b[39m#ðŸ˜‰ polygon to mask. \u001b[39;00m\n\u001b[0;32m    284\u001b[0m seg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(masks)\u001b[39m.\u001b[39mmax(\u001b[39m0\u001b[39m) \u001b[39m#ðŸ˜‰ seg, \u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39;49mopen(join(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_path, \u001b[39mstr\u001b[39;49m(img_ref_data[\u001b[39m'\u001b[39;49m\u001b[39mimage_id\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m))) \u001b[39m#ðŸ˜‰ img\u001b[39;00m\n\u001b[0;32m    287\u001b[0m min_shape \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(img\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]) \u001b[39m#ðŸ˜‰ min_shape. \u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maug_crop: \u001b[39m#ðŸ˜‰ augment the crop, \u001b[39;00m\n",
      "File \u001b[1;32md:\\PhD\\repositories\\clipseg\\.CLIPSEG\\lib\\site-packages\\PIL\\Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3224\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3226\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3227\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3228\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\usuario/datasets/PhraseCut/VGPhraseCut_v0/images/2384227.jpg'"
     ]
    }
   ],
   "source": [
    "a = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".CLIPSEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "830aef9cd6859694268c088411cb058faec24a5a65da1f26d3ab795eccfbf0f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
